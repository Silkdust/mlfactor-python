{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Penalized regressions and sparse hedging for minimum variance portfolios\n",
    "\n",
    "Possible applications of \"regularization\" for linear models:\n",
    "\n",
    "- Improve the *robustness* of factor-based predictive regressions\n",
    "- Fuel an allocation scheme (Han et al., 2019; Rapach and Zhou, 2019)\n",
    "- Improve the quality of mean-variance driven portfolio weights (Stevens, 1998)\n",
    "- General idea: remove noises (at the cost of a possible bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Penalized Regressions\n",
    "\n",
    "### Simple Regressions\n",
    "\n",
    "The classical linear function: $\\boldsymbol{y}=\\boldsymbol{X}\\boldsymbol{\\beta}+\\boldsymbol{\\varepsilon}$. \n",
    "\n",
    "The best choice of $\\boldsymbol{\\varepsilon}$ is naturally the one that *minimizes the error*. A general idea is to minimize the *square errors*: $L=\\boldsymbol{\\varepsilon}^{'}\\boldsymbol{\\varepsilon}=\\sum_i \\varepsilon_i^2$. The loss $L$ is called the sum of squared residuals (*SSR*). Take partial differentiation to get\n",
    "\\begin{align*}\n",
    "\\nabla_{\\boldsymbol{\\beta}} L&=\\frac{\\partial}{\\partial \\boldsymbol{\\beta}}(\\textbf{y}-\\textbf{X}\\boldsymbol{\\beta})'(\\boldsymbol{y}-\\boldsymbol{X}\\boldsymbol{\\beta})=\\frac{\\partial}{\\partial \\boldsymbol{\\beta}}[\\boldsymbol{\\beta}'\\boldsymbol{X}'\\boldsymbol{X}\\boldsymbol{\\beta}-2\\boldsymbol{y}'\\boldsymbol{X}\\boldsymbol{\\beta}] \\\\\n",
    "&=2\\boldsymbol{X}'\\boldsymbol{X}\\boldsymbol{\\beta}  -2\\boldsymbol{X}'\\boldsymbol{y}\n",
    "\\end{align*}\n",
    "so that the first order condition $\\nabla_{\\boldsymbol{\\beta}}=\\mathbf{0}$ is satisfied if $$\\boldsymbol{\\beta}^*=(\\boldsymbol{X}'\\boldsymbol{X})^{-1}\\boldsymbol{X}'\\boldsymbol{y}$$\n",
    "which is known as the **standard ordinary least squares (OLS)** solution of the linear model. Two issues:\n",
    "\n",
    "- Matrix $\\boldsymbol{X}$ with dimensions $I\\times K$. $\\boldsymbol{X}'\\boldsymbol{X}$ can only be inverted if $I$ (*nbs. of rows*) is strictly superior to $K$ (*nbs. of columns*). If there are more predictors than instances then there is no unique value of $\\boldsymbol{\\beta}$ that minimizes the loss."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "406fc061deb33f0aa77d26ddb85a341d574dd3281c000087856ef3a4cde589f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
