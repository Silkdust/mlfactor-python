{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context\n",
    "\n",
    "Blossoming of machine learning:\n",
    "\n",
    "- Data size sufficiently large\n",
    "- Computational Power\n",
    "- Economic Framing: non-linearity in asset pricing, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Portfolio Construction: the Workflow\n",
    "\n",
    "The baseline equation in supervised learning $$\\mathbf{y}=f(\\mathbf{X})+\\varepsilon$$ is translated in financial terms as $$\\mathbf{r}_{t+1,n}=f(\\mathbf{x}_{t,n})+\\varepsilon_{t+1,n}$$ \n",
    "where $f(\\mathbf{x}_{t,n})$ can be viewed as the **expected return** for time $t+1$ computed at time $t$, i.e. $\\mathbb{E}_t[r_{t+1,n}]$. Note that the model is common to all assets ($f$ is not indexed by $n$), thus it shares similarity with panel approaches.\n",
    "\n",
    "How to make accurate predictions?\n",
    "\n",
    "- Gather better data, include some classical predictors\n",
    "- The choice and engineering of inputs are important\n",
    "- An integrated process\n",
    "\n",
    "![FIGURE 2.1 Simplified workflow in ML-based portfolio construction](images\\figure2-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning is No Magic Wand\n",
    "\n",
    "In fact, heuristic guesses are often hard to beat. Below, we sum up some key points that we have learned through our exploratory journey in financial ML:\n",
    "\n",
    "- **Causality** is key. If one is able to identify $X \\rightarrow y$, where $y$ are expected returns, then the problem is solved. Unfortunately, causality is incredibly hard to uncover.\n",
    "- Thus, researchers have most of the time to make do with simple **correlation** patterns, which are far less informative and robust.\n",
    "- Relatedly, financial datasets are extremely **noisy**. It is a daunting task to extract signals out of them. No-arbitrage reasonings imply that if a simple pattern yielded durable profits, it would mechanically and rapidly vanish.\n",
    "- **Data is key**. The inputs given to the models are probably much more important than the choice of the model itself.\n",
    "- **Persistent** series are more likely to unveil enduring patterns.\n",
    "- What matters is to learn from those lapses.\n",
    "\n",
    "Gathering and cleaning data, coding backtests, tuning ML models, testing weighting schemes, debugging, starting all over again: these are all absolutely indispensable steps and tasks that must be repeated indefinitely. There is no sustitute to experience.\n",
    "\n",
    "Finally, this chapter emphasizes two key points:\n",
    "\n",
    "- **Data is key.** Better data often saves your effort for tuning models. The choice of models is probably not as important as \"inputs\".\n",
    "- **Practice makes perfect.** TRY MORE CASES!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "406fc061deb33f0aa77d26ddb85a341d574dd3281c000087856ef3a4cde589f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
