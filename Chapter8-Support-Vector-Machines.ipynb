{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines\n",
    "\n",
    "The origins of SVMs are old (go back to Vapnik and Lerner (1963)). However, their modern treatment was initiated in Boser et al. (1992) and Cortes and Vapnik (1995) (binary classification) and Drucker et al. (1997) (regression). We refer to [these books](http://www.kernel-machines.org/books) for an exhaustive bibliography."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM for Classification\n",
    "\n",
    "Let's consider a simple case for binary classification. In the following figure (8.1), the goal is to find a model that correctly classifies points.\n",
    "\n",
    "A model consists of two weights $\\boldsymbol{w}=(w_1,w_2)$ that load on the variables and create a natural linear separation in the plane.\n",
    "\n",
    "![FIGURE 8.1: Diagram of binary classification with support vectors](images/figure8-1.png)\n",
    "\n",
    "- The red line: a bad classifier (not discriminating different marks)\n",
    "- The blue line: good\n",
    "- The green line: good, but with larger margin\n",
    "\n",
    "What about the grey star?\n",
    "- Given its location, should be a circle\n",
    "- Blue line fails to recognize it\n",
    "- Grey dotted lines: \"margins\"\n",
    "\n",
    "The two margins are computed as the parallel lines that *maximize* the distance between the model and the *closest* points that are correctly classified (on both sides). These points are called **support vectors**, which justifies the name of the technique.\n",
    "\n",
    "The core idea of SVMs is to **maximize the margin**, under the constraint that the classifier does not make any mistake (tries to find the most robust model).\n",
    "\n",
    "Formally, if we numerically define circles as $+1$ and squares as $-1$, a good linear model is expected to satisfy\n",
    "$$\n",
    "\\left\\{\\begin{array}{lll}\n",
    "\\sum_{k=1}^Kw_kx_{i,k}+b \\ge +1 & \\text{ when } y_i=+1 \\\\\n",
    "\\sum_{k=1}^Kw_kx_{i,k}+b \\le -1 & \\text{ when } y_i=-1\n",
    "\\end{array}\\right.\n",
    "$$\n",
    "which can be summarized in compact form as $y_i \\times \\left(\\sum_{k=1}^K w_kx_{i,k}+b \\right)\\ge 1$. \n",
    "\n",
    "Now, the margin between a green model and a support vector on the dashed grey line is equal to $||\\boldsymbol{w}||^{-1}=\\left(\\sum_{k=1}^Kw_k^2\\right)^{-1/2}$.\n",
    "-  This value comes from the fact that the distance between a point $(x_0,y_0)$ and a line parametrized by $ax+by+c=0$ is $d=\\frac{|ax_0+by_0+c|}{\\sqrt{a^2+b^2}}$.\n",
    "Thus, the final problem is\n",
    "$$\\underset{\\boldsymbol{w}, b}{\\text{argmin}} \\ \\frac{1}{2} ||\\boldsymbol{w}||^2 \\ \\text{ s.t. } y_i\\left(\\sum_{k=1}^Kw_kx_{i,k}+b \\right)\\ge 1$$\n",
    "The dual form of this program (see chapter 5 in Boyd and Vandenberghe (2004)) is\n",
    "$$L(\\textbf{w},b,\\boldsymbol{\\lambda})=\n",
    " \\frac{1}{2}||\\textbf{w}||^2 + \\sum_{i=1}^I\\lambda_i\\left(y_i\\left(\\sum_{k=1}^Kw_kx_{i,k}+b \\right)- 1\\right)$$\n",
    "where either $\\lambda_i=0$ or $y_i\\left(\\sum_{k=1}^Kw_kx_{i,k}+b \\right)= 1$. Thus, **only some points** will matter in the solution (the so-called **support vectors**).\n",
    "\n",
    "The first order conditions:\n",
    "$$\\frac{\\partial L}{\\partial \\boldsymbol{w}}L(\\boldsymbol{w},b,\\boldsymbol{\\lambda})=\\textbf{0}, \\quad \\frac{\\partial L}{\\partial b}L(\\boldsymbol{w},b,\\boldsymbol{\\lambda})=0$$\n",
    "where the first condition leads to\n",
    "$$\\boldsymbol{w}^*=\\sum_{i=1}^I \\lambda_i u_i \\boldsymbol{x}_i$$\n",
    "The solution is indeed a linear form of the features, but only some points are taken into account.\n",
    "\n",
    "Naturally, this problem becomes *infeasible* whenever the condition cannot be satisfied (a simple line cannot perfectly separate the labels), i.e., logically, *not linearly separable*. This complicates the process and it's possible to resort to a trick by **adding correction variables** that allow the conditions to be met:\n",
    "$$\\left\\{\\begin{array}{lll}\n",
    "\\sum_{k=1}^Kw_kx_{i,k}+b \\ge +1-\\xi_i & \\text{ when } y_i=+1 \\\\\n",
    "\\sum_{k=1}^Kw_kx_{i,k}+b \\le -1+\\xi_i & \\text{ when } y_i=-1,\n",
    "\\end{array}\\right.$$\n",
    "where the $\\xi_i$ are positive so-called **slack variables** that make the conditions feasible and illustrated by the following figure:\n",
    "\n",
    "![FIGURE 8.2: Diagram of binary classification with SVM - linearly inseparable data](images/figure8-2.png)\n",
    "\n",
    "The optimization program then becomes\n",
    "$$\\underset{\\boldsymbol{w},b, \\boldsymbol{\\xi}}{\\text{argmin}} \\ \\frac{1}{2} ||\\boldsymbol{w}||^2+C\\sum_{i=1}^I\\xi_i \\ \\text{ s.t. } \\left\\{ y_i\\left(\\sum_{k=1}^Kw_k\\phi(x_{i,k})+b \\right)\\ge 1-\\xi_i \\ \\text{ and } \\ \\xi_i\\ge 0, \\ \\forall i  \\right\\},$$\n",
    "where the parameter $C>0$ tunes the cost of mis-classification: as $C$ increases, errors become more penalizing.\n",
    "\n",
    "The program can also be generalized to *non-linear* models with kernel $\\phi$ applied to the input points $x_{i,k}$. The following figures show non-linear kernels can help cope with patterns more complex than straight lines. \n",
    "\n",
    "Once the weights $\\boldsymbol{w}$ and bias $b$ are set via training, a prediction for a new vector $\\boldsymbol{x}_j$ is simply made by $\\sum_{k=1}^K w_k \\phi(x_{j,k})+b$ and choosing the class based on the sign.\n",
    "\n",
    "![FIGURE 8.3: Examples of nonlinear kernels](images/figure8-3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM for Regression\n",
    "\n",
    "The ideas of SVM for classification can be transposed to regression but the role of the margin is different. One general formulation is\n",
    "\\begin{align*}\n",
    "\\underset{\\boldsymbol{w},b, \\boldsymbol{\\xi}}{\\text{argmin}} \\  & \\frac{1}{2} ||\\boldsymbol{w}||^2+C\\sum_{i=1}^I\\left(\\xi_i+\\xi_i^* \\right)\\\\\n",
    " \\text{ s.t. }&  \\sum_{k=1}^Kw_k\\phi(x_{i,k})+b -y_i\\le \\varepsilon+\\xi_i \\\\\n",
    "&  y_i-\\sum_{k=1}^Kw_k\\phi(x_{i,k})-b \\le \\varepsilon+\\xi_i^* \\\\\n",
    "&\\xi_i,\\xi_i^*\\ge 0, \\ \\forall i\n",
    "\\end{align*}\n",
    "as illustrated in the following figure. The user specifies a **margin** $\\varepsilon$ and the model will try to find the linear (or kernel transformation) relationship between the labels $y_i$ and the input $\\boldsymbol{x}_i$. Just as in the classification task, if the data points are inside the 'strip', the slack variables $\\xi_i$ and $\\xi_i^*$ are 0.\n",
    "\n",
    "When the points violate the threshold, the objective function is penalized (by $\\xi_i$ and $\\xi_i^*$), and setting a large $\\varepsilon$ leaves room for more error. Once the model has been trained, a prediction for $\\boldsymbol{x}_j$ is simply $\\sum_{k=1}^K w_k \\phi(x_{j,k})+b$.\n",
    "\n",
    "![FIGURE 8.4: Diagram of regression SVM](images/figure8-4.png)\n",
    "\n",
    "The algorithm\n",
    "- Minimizes the sum of squared weights $||\\boldsymbol{w}||^2$ subject to the error being small enough.\n",
    "- The \"opposite\" of the penalized linear regressions which seek to minimize the error, subject to the weights being small enough.\n",
    "\n",
    "One may refer to Chang and Lin (2011) for more details on the SVM zoo.\n",
    "Another reference library coded by `C` and `C++` IS `LIBSVM`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice\n",
    "\n",
    "For the sake of consistency we will use `scikit-learn`'s implementation of SVM in the following codes. In the implementation of `LIBSVM`, the package requires to specify the label and features separately. For this reason, we recycle the variables used for the boosted trees. Moreover, the training being slow, we perform it on a subsample of these sets (first thousand instances)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data as in Chapter 6\n",
    "import pandas as pd\n",
    "\n",
    "data_ml = pd.read_pickle('./data/data_ml.pkl')\n",
    "X = data_ml[data_ml.columns[2:95]]\n",
    "y = data_ml['R1M_Usd']\n",
    "\n",
    "separation_date = pd.to_datetime('2014-01-15')\n",
    "training_sample = data_ml[data_ml['date'] < separation_date]\n",
    "test_sample = data_ml[data_ml['date'] > separation_date]\n",
    "\n",
    "r1m_use_quantiles = (training_sample['R1M_Usd'].quantile(0.2), training_sample['R1M_Usd'].quantile(0.8))\n",
    "features_short = [\"Div_Yld\", \"Eps\", \"Mkt_Cap_12M_Usd\", \"Mom_11M_Usd\", \"Ocf\", \"Pb\", \"Vol1Y_Usd\"]\n",
    "\n",
    "training_samples_svm = training_sample[(training_sample['R1M_Usd'] < r1m_use_quantiles[0]) | (training_sample['R1M_Usd'] > r1m_use_quantiles[1])].reset_index().drop(columns = 'index')\n",
    "\n",
    "training_features_svm = training_samples_svm[features_short][:1000]\n",
    "training_label_svm = training_samples_svm['R1M_Usd'][:1000]\n",
    "test_features_svm = test_sample[features_short]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVR(C=0.1, gamma=0.5)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVR</label><div class=\"sk-toggleable__content\"><pre>SVR(C=0.1, gamma=0.5)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVR(C=0.1, gamma=0.5)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "\n",
    "fit_svm = SVR(kernel='rbf', C=0.1, epsilon=0.1, gamma=0.5)\n",
    "fit_svm.fit(training_features_svm, training_label_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03720290963990271"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.mean(np.power(fit_svm.predict(test_features_svm) - test_sample['R1M_Usd'], 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5270339562443026"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(fit_svm.predict(test_features_svm) * test_sample['R1M_Usd'] > 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are slightly better than those of the boosted trees. All parameters are completely arbitrary, especially the choice of the kernel. We finally turn to a classification example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(C=0.2, coef0=0.3, gamma=0.5, kernel=&#x27;sigmoid&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(C=0.2, coef0=0.3, gamma=0.5, kernel=&#x27;sigmoid&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC(C=0.2, coef0=0.3, gamma=0.5, kernel='sigmoid')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "training_label_svm_C = training_samples_svm['R1M_Usd_C'][:1000]\n",
    "\n",
    "fit_svm_C = SVC(kernel='sigmoid', gamma=0.5, coef0=0.3, C=0.2)\n",
    "fit_svm_C.fit(training_features_svm, training_label_svm_C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49628247493163175"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(fit_svm_C.predict(test_features_svm) == test_sample['R1M_Usd_C'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both the small training sample and the arbitrariness in our choice of the parameters may explain why the predictive accuracy is so poor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding Exercises\n",
    "\n",
    "1. From the simple example shown above, extend SVM models to other kernels and discuss the impact on the fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernels = ['linear', 'poly', 'rbf', 'sigmoid']\n",
    "def evaluate_kernel_fit(kernel, \n",
    "                        training_features_svm, training_label_svm, \n",
    "                        test_features_svm, test_label_svm):\n",
    "    # Fit a SVM for regression with the specified kernel\n",
    "    fit_svm = SVR(kernel=kernel, C=0.1, epsilon=0.1, gamma=0.5)\n",
    "    fit_svm.fit(training_features_svm, training_label_svm)\n",
    "\n",
    "    mse = np.mean(np.power(fit_svm.predict(test_features_svm) - test_label_svm, 2))\n",
    "    hit_ratio = np.mean(fit_svm.predict(test_features_svm) * test_label_svm > 0)\n",
    "    return mse, hit_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'linear': (0.03705349279047895, 0.5292701686417502),\n",
       " 'poly': (0.037330909903200316, 0.5269769826800365),\n",
       " 'rbf': (0.03720290963990271, 0.5270339562443026),\n",
       " 'sigmoid': (0.2903700864598458, 0.49175307657247036)}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit_results = {kernel: evaluate_kernel_fit(kernel, training_features_svm, \n",
    "               training_label_svm, test_features_svm, test_sample['R1M_Usd']) \n",
    "               for kernel in kernels}\n",
    "fit_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first two kernels yield the best fit, while the last one should be avoided. Note that apart from the linear kernel, all other options require parameters. We have used the default ones, which may explain the poor performance of some nonlinear kernels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Train a vanilla SVM model with labels being the 12-month forward (i.e., future) return and evaluate it on the testing sample. Do the same with a simple random forest. Compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.24691967843508394, 0.48504443938012765)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_label_svm_12 = training_samples_svm['R12M_Usd'][:1000]\n",
    "evaluate_kernel_fit('linear', training_features_svm, training_label_svm_12,\n",
    "                    test_features_svm, test_sample['R12M_Usd'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vanilla model seems unsatisfying when dealing with 12-month forward return. Let's try a simple random forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.3208760528389372, 0.48817798541476753)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "fit_rf_12 = RandomForestRegressor(n_estimators=100)\n",
    "fit_rf_12.fit(training_features_svm, training_label_svm_12)\n",
    "\n",
    "mse_rf = np.mean(np.power(fit_rf_12.predict(test_features_svm) - test_sample['R12M_Usd'], 2))\n",
    "hit_ratio_rf = np.mean(fit_rf_12.predict(test_features_svm) * test_sample['R12M_Usd'] > 0)\n",
    "mse_rf, hit_ratio_rf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MSE from the simple random forest seems to be more unsatisfying than a vanilla SVM with linear kernel. However, the hit ratio does improve a little. It's clear that the small training sample, the arbitrariness in the choice of the parameters, and the forecasting horizon could explain the poor performances of both models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "406fc061deb33f0aa77d26ddb85a341d574dd3281c000087856ef3a4cde589f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
